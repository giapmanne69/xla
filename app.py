import os
import cv2
import time
import numpy as np
from flask import Flask, render_template, request, jsonify, url_for
from pathlib import Path

# --- TH∆Ø VI·ªÜN YOLO & SAHI ---
from ultralytics import YOLO
from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction
from sahi.utils.cv import visualize_object_predictions

# ==========================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG
# ==========================================
app = Flask(__name__)

UPLOAD_FOLDER = 'static/uploads'
RESULT_FOLDER = 'static/results'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(RESULT_FOLDER, exist_ok=True)

# ƒê∆∞·ªùng d·∫´n model (S·ª≠a l·∫°i cho ƒë√∫ng m√°y b·∫°n)
MODEL_PATH = 'app\dataset\clahe_dataset\\best.pt' 

print(f"‚è≥ ƒêang t·∫£i m√¥ h√¨nh t·ª´ {MODEL_PATH}...")

# --- LOAD MODEL 1: CHO VIDEO ---
try:
    yolo_model = YOLO(MODEL_PATH)
    print("‚úÖ Model YOLO chu·∫©n (cho Video): S·∫µn s√†ng!")
except Exception as e:
    print(f"‚ùå L·ªói load YOLO: {e}")
    exit()

# --- LOAD MODEL 2: CHO ·∫¢NH (SAHI) ---
try:
    sahi_model = AutoDetectionModel.from_pretrained(
        model_type='yolov11', 
        model_path=MODEL_PATH,
        confidence_threshold=0.4, 
        device="cuda:0" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else "cpu"
    )
    print("‚úÖ Model SAHI (cho ·∫¢nh): S·∫µn s√†ng!")
except Exception as e:
    print(f"‚ùå L·ªói load SAHI: {e}")
    sahi_model = None

# ==========================================
# 2. THU·∫¨T TO√ÅN TI·ªÄN X·ª¨ L√ù (CH·ªà D√ôNG CHO VIDEO)
# ==========================================
def apply_clahe_lab(img):
    """ Ch·ªâ d√πng cho Video ƒë·ªÉ tracking t·ªët h∆°n """
    try:
        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        cl = clahe.apply(l)
        limg = cv2.merge((cl, a, b))
        final_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)
        return final_img
    except:
        return img

# ==========================================
# 3. X·ª¨ L√ù ·∫¢NH (SAHI + FIX M√ÄU)
# ==========================================
def process_image_with_sahi(img_path, save_path):
    # 1. ƒê·ªçc ·∫£nh b·∫±ng OpenCV (ƒê·ªãnh d·∫°ng BGR)
    img_bgr = cv2.imread(img_path)
    
    # --- FIX L·ªñI M√ÄU T·∫†I ƒê√ÇY ---
    # Chuy·ªÉn BGR -> RGB tr∆∞·ªõc khi ƒë∆∞a v√†o SAHI
    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    
    if sahi_model:
        # 2. D·ª± ƒëo√°n b·∫±ng SAHI tr√™n ·∫£nh RGB
        result = get_sliced_prediction(
            img_rgb,  # ƒê∆∞a ·∫£nh RGB v√†o
            sahi_model,
            slice_height=320,  
            slice_width=320,
            overlap_height_ratio=0.2,
            overlap_width_ratio=0.2,
            verbose=0
        )
        
        # 3. V·∫Ω k·∫øt qu·∫£
        visualization_result = visualize_object_predictions(
            img_rgb, # V·∫Ω l√™n ·∫£nh RGB g·ªëc
            object_prediction_list=result.object_prediction_list,
            rect_th=2,
            text_size=0.6,
            text_th=2
        )
        final_img_rgb = visualization_result["image"]
        
        # 4. Chuy·ªÉn ng∆∞·ª£c RGB -> BGR ƒë·ªÉ l∆∞u file b·∫±ng OpenCV
        final_img_bgr = cv2.cvtColor(final_img_rgb, cv2.COLOR_RGB2BGR)
        
    else:
        # Fallback YOLO th∆∞·ªùng
        res = yolo_model.predict(img_bgr, imgsz=640, conf=0.5)
        final_img_bgr = res[0].plot()

    # 5. L∆∞u ·∫£nh (OpenCV c·∫ßn BGR)
    cv2.imwrite(save_path, final_img_bgr)

# ==========================================
# 4. X·ª¨ L√ù VIDEO (YOLO TRACK + CLAHE)
# ==========================================
def process_video_tracking(video_path, output_path):
    cap = cv2.VideoCapture(video_path)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v') 
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Video: CLAHE + Tracking
        processed_frame = apply_clahe_lab(frame)
        results = yolo_model.track(processed_frame, imgsz=640, conf=0.5, persist=True, tracker="bytetrack.yaml", verbose=False)
        
        # V·∫Ω
        annotated_frame = results[0].plot()
        out.write(annotated_frame)

    cap.release()
    out.release()

# ==========================================
# 5. ROUTES
# ==========================================
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    if 'file' not in request.files:
        return jsonify({'error': 'Kh√¥ng t√¨m th·∫•y file'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'Ch∆∞a ch·ªçn file'}), 400

    filename = file.filename
    file_path = os.path.join(UPLOAD_FOLDER, filename)
    file.save(file_path)

    is_image = filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))
    timestamp = int(time.time())
    
    if is_image:
        result_filename = f"res_{timestamp}_{filename}"
        result_path = os.path.join(RESULT_FOLDER, result_filename)
        # G·ªåI H√ÄM X·ª¨ L√ù ·∫¢NH
        process_image_with_sahi(file_path, result_path)
        ftype = 'image'
    else:
        result_filename = f"res_{timestamp}_{Path(filename).stem}.mp4"
        result_path = os.path.join(RESULT_FOLDER, result_filename)
        # G·ªåI H√ÄM X·ª¨ L√ù VIDEO
        process_video_tracking(file_path, result_path)
        ftype = 'video'

    return jsonify({
        'status': 'success',
        'type': ftype,
        'result_url': url_for('static', filename=f'results/{result_filename}')
    })

if __name__ == '__main__':
    print("üåç Web App SAHI ƒëang ch·∫°y t·∫°i: http://localhost:5000")
    app.run(host='0.0.0.0', port=5000, debug=True)