import os
import cv2
import time
import numpy as np
from flask import Flask, render_template, request, jsonify, url_for
from pathlib import Path

# --- TH∆Ø VI·ªÜN YOLO & SAHI ---
from ultralytics import YOLO
from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction
from sahi.utils.cv import visualize_object_predictions

# ==========================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG
# ==========================================
app = Flask(__name__)

UPLOAD_FOLDER = 'static/uploads'
RESULT_FOLDER = 'static/results'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(RESULT_FOLDER, exist_ok=True)

# ƒê∆∞·ªùng d·∫´n model c·ªßa b·∫°n (∆∞u ti√™n .pt v√¨ SAHI h·ªó tr·ª£ .pt t·ªët h∆°n onnx)
MODEL_PATH = 'best.pt' # H√£y ch·∫Øc ch·∫Øn file n√†y n·∫±m c√πng th∆∞ m·ª•c
# MODEL_PATH = r"D:\Duong\Dan\Cua\Ban\best.pt" 

print(f"‚è≥ ƒêang t·∫£i m√¥ h√¨nh t·ª´ {MODEL_PATH}...")

# --- LOAD MODEL 1: CHO VIDEO (YOLO CHU·∫®N - T·ªêC ƒê·ªò CAO) ---
try:
    yolo_model = YOLO(MODEL_PATH)
    print("‚úÖ Model YOLO chu·∫©n (cho Video): S·∫µn s√†ng!")
except Exception as e:
    print(f"‚ùå L·ªói load YOLO: {e}")
    exit()

# --- LOAD MODEL 2: CHO ·∫¢NH (SAHI - ƒê·ªò CH√çNH X√ÅC C·ª∞C CAO) ---
try:
    sahi_model = AutoDetectionModel.from_pretrained(
        model_type='yolov8', # SAHI d√πng chu·∫©n v8 t∆∞∆°ng th√≠ch v11
        model_path=MODEL_PATH,
        confidence_threshold=0.4, # Ng∆∞·ª°ng t·ª± tin
        device="cuda:0" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else "cpu"
    )
    print("‚úÖ Model SAHI (cho ·∫¢nh): S·∫µn s√†ng!")
except Exception as e:
    print(f"‚ùå L·ªói load SAHI: {e}")
    # N·∫øu l·ªói SAHI th√¨ ta s·∫Ω fallback v·ªÅ d√πng yolo_model th∆∞·ªùng
    sahi_model = None

# ==========================================
# 2. THU·∫¨T TO√ÅN TI·ªÄN X·ª¨ L√ù (CLAHE LAB)
# ==========================================
def apply_clahe_lab(img):
    """
    TƒÉng c∆∞·ªùng t∆∞∆°ng ph·∫£n th√¥ng minh: Gi·ªØ m√†u, tƒÉng n√©t v√πng t·ªëi.
    """
    try:
        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        cl = clahe.apply(l)
        limg = cv2.merge((cl, a, b))
        final_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)
        return final_img
    except:
        return img

# ==========================================
# 3. X·ª¨ L√ù ·∫¢NH (D√ôNG SAHI + CLAHE)
# ==========================================
def process_image_with_sahi(img_path, save_path):
    # 1. ƒê·ªçc ·∫£nh
    original_img = cv2.imread(img_path)
    
    # 2. Ti·ªÅn x·ª≠ l√Ω (CLAHE)
    processed_img = apply_clahe_lab(original_img)
    
    # 3. D·ª± ƒëo√°n b·∫±ng SAHI (C·∫Øt l√°t)
    if sahi_model:
        # SAHI t·ª± ƒë·ªông c·∫Øt ·∫£nh th√†nh c√°c m·∫£nh nh·ªè (slice) ƒë·ªÉ soi v·∫≠t th·ªÉ nh·ªè
        result = get_sliced_prediction(
            processed_img,
            sahi_model,
            slice_height=320,  # K√≠ch th∆∞·ªõc m·ªói m·∫£nh c·∫Øt (c√†ng nh·ªè c√†ng soi k·ªπ)
            slice_width=320,
            overlap_height_ratio=0.2,
            overlap_width_ratio=0.2,
            verbose=0
        )
        
        # 4. V·∫Ω k·∫øt qu·∫£ l√™n ·∫£nh
        # SAHI c√≥ h√†m visualize ri√™ng, ta xu·∫•t ra numpy array ƒë·ªÉ l∆∞u b·∫±ng cv2
        visualization_result = visualize_object_predictions(
            processed_img,
            object_prediction_list=result.object_prediction_list,
            rect_th=2,
            text_size=0.6,
            text_th=2
        )
        final_img = visualization_result["image"]
        # SAHI visualize tr·∫£ v·ªÅ RGB, c·∫ßn convert v·ªÅ BGR ƒë·ªÉ OpenCV l∆∞u ƒë√∫ng m√†u
        final_img = cv2.cvtColor(final_img, cv2.COLOR_RGB2BGR)
        
    else:
        # Fallback: N·∫øu kh√¥ng load ƒë∆∞·ª£c SAHI th√¨ d√πng YOLO th∆∞·ªùng
        print("‚ö†Ô∏è ƒêang d√πng YOLO th∆∞·ªùng cho ·∫£nh (Do SAHI ch∆∞a load)")
        res = yolo_model.predict(processed_img, imgsz=640, conf=0.5)
        final_img = res[0].plot()

    # 5. L∆∞u ·∫£nh
    cv2.imwrite(save_path, final_img)

# ==========================================
# 4. X·ª¨ L√ù VIDEO (D√ôNG YOLO TRACK + CLAHE)
# ==========================================
def process_video_tracking(video_path, output_path):
    """
    Video d√πng YOLO chu·∫©n ƒë·ªÉ ƒë·∫£m b·∫£o FPS, kh√¥ng d√πng SAHI v√¨ s·∫Ω r·∫•t lag.
    """
    cap = cv2.VideoCapture(video_path)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v') 
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # 1. CLAHE
        processed_frame = apply_clahe_lab(frame)

        # 2. Tracking (YOLOv11 Standard)
        # TƒÉng imgsz l√™n 1280 (n·∫øu m√°y ch·ªãu n·ªïi) ƒë·ªÉ b√π ƒë·∫Øp vi·ªác kh√¥ng d√πng SAHI
        results = yolo_model.track(processed_frame, imgsz=640, conf=0.5, persist=True, tracker="bytetrack.yaml", verbose=False)
        
        # 3. V·∫Ω & Ghi
        annotated_frame = results[0].plot()
        out.write(annotated_frame)

    cap.release()
    out.release()

# ==========================================
# 5. ROUTES
# ==========================================
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    if 'file' not in request.files:
        return jsonify({'error': 'Kh√¥ng t√¨m th·∫•y file'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'Ch∆∞a ch·ªçn file'}), 400

    filename = file.filename
    file_path = os.path.join(UPLOAD_FOLDER, filename)
    file.save(file_path)

    is_image = filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))
    timestamp = int(time.time())
    
    if is_image:
        result_filename = f"res_{timestamp}_{filename}"
        result_path = os.path.join(RESULT_FOLDER, result_filename)
        # G·ªåI H√ÄM SAHI CHO ·∫¢NH
        process_image_with_sahi(file_path, result_path)
        ftype = 'image'
    else:
        result_filename = f"res_{timestamp}_{Path(filename).stem}.mp4"
        result_path = os.path.join(RESULT_FOLDER, result_filename)
        # G·ªåI H√ÄM TRACKING CHO VIDEO
        process_video_tracking(file_path, result_path)
        ftype = 'video'

    return jsonify({
        'status': 'success',
        'type': ftype,
        'result_url': url_for('static', filename=f'results/{result_filename}')
    })

if __name__ == '__main__':
    print("üåç Web App SAHI ƒëang ch·∫°y t·∫°i: http://localhost:5000")
    app.run(host='0.0.0.0', port=5000, debug=True)